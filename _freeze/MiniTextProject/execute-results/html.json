{
  "hash": "d0dc214b641984779998fe3781001655",
  "result": {
    "markdown": "---\ntitle: \"Mini Text Project\"\nsidebar: false\nformat:\n  html: default\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n## Unifished framework of MP4\n\n\n# Loading the Data from github broke the file is 160 mb and I think I overwhelmed the website, so this everything is eval = false right now\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(jsonlite)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'jsonlite' was built under R version 4.3.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidytext)\nlibrary(textdata)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'textdata' was built under R version 4.3.3\n```\n:::\n\n```{.r .cell-code}\nlibrary(wordcloud)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'wordcloud' was built under R version 4.3.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: RColorBrewer\n```\n:::\n\n```{.r .cell-code}\nlibrary(wordcloud2)\nlibrary(viridis)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: viridisLite\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggthemes)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read_json(\"https://github.com/leinstay/steamdb/raw/refs/heads/main/steamdb.json\")\n\n\ndata2 <- as.data.frame(do.call(rbind, data))\n\n\n str_view(data2$name, \"\\\\w* \\\\d\\\\b\") |> print(n=Inf) \n str_view(data2$name, \"\")\n \n \n data2 <- data2 |> separate_wider_delim(published_store, delim = \"-\",\n          names = c(\"publishedyear\", \"publishedmonth\", \"publishedday\"), too_few = \"debug\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n data2 |> drop_na(publishedyear) |> \n   filter(publishedyear > 2006,\n          publishedyear != \"NULL\") |>\n   mutate(sequel = str_detect(name, \"\\\\w* \\\\d\\\\d?\\\\b\")) |> \n   group_by(publishedyear) |> summarize(num_sequel = sum(sequel),\n                                        num_total = n(),\n                                        prop_sequel = mean(sequel)) |> print(n=100)\n\n data2 |> drop_na(publishedyear) |> \n   filter(publishedyear > 2006,\n          publishedyear != \"NULL\") |>\n   mutate(sequel = str_detect(name, \"(..).*\\\\1 \")) \n \n  \n str_subset(data2$name, \"(..\\\\w*) \\\\1\")\n \n   group_by(publishedyear) |> summarize(num_sequel = sum(sequel),\n                                        num_total = n(),\n                                        prop_sequel = mean(sequel)) |> print(n=100)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#the json data waas very broken, not sure if these count as strings and regular expressions for my 3, but basically i'm just tricking r into turning into proper text\n descriptions <- data2 |> \n   mutate(names = str_extract(name, \".*\"),\n          descriptions = str_extract(description, \".*\")) |>\n   select(descriptions, names, publishedyear)\n \n tidy_descriptions <- descriptions |>\n  mutate(line = row_number()) |>\n  unnest_tokens(word, descriptions, token = \"words\")\n   \nsmart_stopwords <- get_stopwords(source = \"smart\")\n\nfont_stopwords <- tibble(\n  word = c(\"br\",\"li\",\"strong\",\"ul\",\"quot\", \"game\"), \n  lexicon = \"font\")\n\n\n#top 20 words of all times\ntidy_descriptions |>\n   filter(publishedyear != \"NULL\") |>\n  anti_join(smart_stopwords) |>\n  anti_join(font_stopwords) |>\n  count(word, sort = TRUE) |>\n  filter(word != \"NA\") |>\n  slice_max(n, n = 20) |>\n  ggplot(aes(fct_reorder(word, n), n)) +\n  geom_col() +\n  coord_flip()\n\n#top 10 word for each year (I tried so hard to order them for each year but it doesn't work )\ntidy_descriptions |>\n   filter(publishedyear != \"NULL\",\n          publishedyear > 2006) |>\n  anti_join(smart_stopwords) |>\n  anti_join(font_stopwords) |>\n  group_by(publishedyear, word) |>\n  summarize(n =n()) |> \n  arrange(publishedyear, desc(n)) |>\n  slice_max(n, n = 10) |> \n  ggplot(aes(fct_reorder(word, n), n, fill = publishedyear)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ publishedyear, scales = \"free\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwords <- tidy_descriptions |>\n  anti_join(stop_words) |>\n  anti_join(font_stopwords) |>\n  count(word) |>\n  filter(word != \"NA\") |>\n  arrange(desc(n))\n\n# word cloud of all years\nwordcloud(\n  words = words$word, \n  freq = words$n, \n  max.words = 200, \n  random.order = FALSE, \n  rot.per = 0.35,\n  scale = c(3.5, 0.25),\n  colors = brewer.pal(9, \"Dark2\"))\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}