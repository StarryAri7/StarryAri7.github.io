[
  {
    "objectID": "Goshawks_Pres.html",
    "href": "Goshawks_Pres.html",
    "title": "Northern Goshawks Observations in California",
    "section": "",
    "text": "Disclaimer: Throughout this paper we refer to Northern Goshawks and American Goshawks somewhat interchangibly. Until this summer, Northern Goshawks were a single species, wheras now they have split into American and Eurasian Goshawk Subspecies. The study spans before and after the split, but only ever focused on American Goshawks.\n\n\nIntroduction:\n\nThe Institute for Bird Populations has conducted population surveys for American goshawks in the Stanislaus and Plumas National Forests for a number of years. The American goshawk is a hawk with short wings and a tail it uses as a rudder for increased agility. They live in large expanses of forests and are difficult to find. When you do find one though, they are highly territorial and quickly will let you know that you should leave them alone by using defensive calls and sometimes dive-bombing.\n\n\nThe USDA has the American Goshawk listed as a sensitive species. There is, however, a history of organizations petitioning to have the species listed as endangered due to increasing loss of mature forests that make up their habitat. Last summer, Emma worked as a field technician in the Stanislaus National Forest with IBP, working with a team throughout the summer to collect data for their goshawk project. We decided to look at some of the data from the project to see if we could find any patterns or come up with any interesting questions from it. The main question guiding us through this project is, which evidence of goshawks is most correlated with (or attracted to) active goshawk nests?\n\n\n- First we must load in our data. We received goshawks_sf and PACBound from Lynn Schofield, a Biologist with the Insitute for Bird Populations. We found NatFors_sf on the USDA website.\n\n\n- We then made a couple of conversions in order to make the data work for us. To use the goshawks_sf dataset for our tests we need to convert the data from its native CRS in latitude and longitude to a new CRS in meters. We chose UTM zone 10 as the CRS for our data as it falls within that zone and is measured in meters.\n\n\nWe also filtered out a variety of variables for later. The Plum and Stan forest bounds will be used in our map at the end, and the MGosObs filters remove a variety of variables that are broken and not of use to our analysis.\n\n\nSimple feature collection with 354 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 641553.2 ymin: 4213818 xmax: 774266.3 ymax: 4457055\nProjected CRS: WGS 84 / UTM zone 10N\n# A tibble: 354 × 9\n   Obs_Type Forest PAC_Name NOGO_Confi Survey_Dat Calc_Nest_ Calc_Nest1 Comments\n * &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;      &lt;date&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;   \n 1 ActiveN… Stani… Cow Crk… Yes        2021-06-08       38.2      -120.  &lt;NA&gt;   \n 2 ActiveN… Stani… SNF4     Yes        2021-07-24       38.2      -120. \"Saw th…\n 3 ActiveN… Stani… Soap Crk Yes        2021-08-12       38.2      -120. \"Adult …\n 4 ActiveN… Stani… Campood… Yes        2021-08-05       38.3      -120. \"Fledge…\n 5 ActiveN… Stani… Tunnel … Yes        2022-06-21       38.2      -120.  &lt;NA&gt;   \n 6 ActiveN… Stani… Soap Crk Yes        2022-07-20       38.2      -120. \"Tree w…\n 7 ActiveN… Stani… Herring… Yes        2022-07-31       38.2      -120. \"Nest: …\n 8 ActiveN… Stani… Smoothw… Yes        2022-07-28       38.3      -120. \"Commen…\n 9 ActiveN… Stani… Smoothw… Yes        2023-06-01       38.3      -120. \"Split …\n10 ActiveN… Stani… Skull C… Yes        2023-06-21       38.3      -120. \"Follwe…\n# ℹ 344 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\n\n\nWe had to play around with the data to make sure it was representing reality correctly. We quickly noticed that we had a few duplicate ‘active nest’ data points, and were wondering if they were the same nest marked twice or different nests that were really close to each other.\n\n\n\n\n\n\n\n\n\n\n\nWe created a “Year” column from our mm/dd/yy date format then looked at a graph that gave different colors to nests found in different years. From this we concluded that they were in fact just the same nest from different years.\n\n\n\n\n\n\n\n\n\n\n\nWe then simply searched through the dataset to find which columns contained the older of the 2 pairs of data and removed the duplicate nests using the slice() function.\n\n\nZoom: 10\n\n\n\n\n\n\n\n\n\n\n\nWe also made an interactive map with all the Goshawk related observational data. The blue and purple outlines show Stanislaus and Plumas forests respectively. The red outlines within the Stanislaus are survey areas known as PACs. Sadly the study is being done differently in Plumas Forest, and not as far progressed, so there are no PACs, it’s less specific, and there is less data to analyze at this time. We will only be further analyzing the Stanislaus Forest PAC data for this reason. Lastly for the map, you will notice a layers section on the left side of the view panel– we encourage you to play around with removing and adding layers to see how they cluster! Active Nests are our variable of interest, and we looked at the other 8 variables to try to tell how predictive they are of an active nest. Though we will go on to test these correlations below, the map provides a fun, engaging, and visual way to look at the observations collected.\n\n\ntmap mode set to interactive viewing\n\n\n\n\n\n\n\n\n- Bounding posed another early challenge in our analysis. Due to the ongoing nature of the study and unsurity about the scale of what was studied and what wasn’t studied in each forest we had to make some assumptions and pretty quickly throw out ideas of doing a standard density plot and prediction analysis. To start a rough analysis we created our own bounding boxes by drawing a line around where our points were. We later also made a bounding with all the points in one ‘box’, and a bounding with the 2 forests in two seperate boundings but within one window.\n\n\nAfter bounding the data, we needed to go through the process of turning our data from sf files into ppp objects for analysis of our point pattern data. We isolated the specific locations and variables of interest for each of our 4 different location types, turned the bounding boxes into windows, and combined the datapoints with the windows to make a ppp object for Plumas, Stanislaus, Both in separate borders, and All in a single border. A keen observer will also notice the entrance of PAC.ppp. This is a premade bounding dataset we recieved last (seen earlier in the interactive map, displayed in red), which helps limit the data to the more specific regions studied within the Stanislaus forest.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, we went through a lot of iterations on this project: first starting with the separate Plumas and Stanislaus window, then creating the All bound and the Both separately, before finally landing on the PAC data, which is by far the most reliable, as we will see below.\n\n\nHere we can see the difference between the same test (clustering between active nests and molted feathers) across the 5 different bounding windows, offering a plethora of different answers. While all confidently show clustering, due to the PAC data being the only dataset that limits the scope of our analysis to the specific regions that were studied, we can output the most accurate data, especially at further distances (which should likely be random, as seen in the first plot). We used Kcross to analyze most of the data in our study as our research question revolves around the correlation between 2 variables, an indiciative observation, and the likelihood that it is correlated to a nest being nearby. Kcross tests just that, to see if 2 variables are attracted, randomly assorted, or repelled across a wide range of distance radii. The envelope (gray area) represents an area where the 2 variables of interest are just randomly assorted to chance, while being above the envelope indicates a notable attraction at that distance, and below the envelope indicates a notable repellence of the 2 variables at that distance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut of all our types of evidence, (Auditory, Whitewash, Seen and Heard, Visual, Molted Feather, Prey Remains, Inactive Nest, Plucking Post) we wanted to see which was the most likely to correlate with the actual presence of an active nest. This information could be helpful for researchers when they are trying to determine the most efficient methods for surveying.\n\n\n\nTypes of Evidence:\n\nActive Nest: Observance of a Northern Goshawk Nest that was actively in use.\n\n\nAuditory: Observer heard a Northern Goshawk call\n\n\nVisual: Observer saw a Northern Goshawk\n\n\nSeen and Heard: Observer saw a Northern Goshawk and heard it’s call\n\n\nMolted Feather: Observance of Northern Goshawk feathers shed onto the ground\n\n\nWhitewash: Observance of Northern Goshawk poop. Observers specifically looked for raptor whitewash which is distinctive from that of smaller birds due to a notably larger splatter.\n\n\nInactive Nest: Observance of a Northern Goshawk Nest from a previous breeding season that was no longer in use.\n\n\nPrey Remains: Observance of remains of prey that Northern Goshawks are known to eat. Examples include feathers from smaller birds and parts of small mammals left behind from a raptor meal.\n\n\nPlucking Post: Observance of a stump or similar structure used by raptors to strip feathers or fur from their prey.\n\n\n\nK-Cross\n\nBecause we’re looking at point-pattern data and the nature of our bounding boxes (even with the PAC data), we decided to largely evaluate using a Kcross function. Due to the evenness of the forest in scale and the limits of our data, we chose to assume homogeneity for our tests, with a limit of 2000 meters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusions and Reflections:\n\nLooking at our 7 different variables, we can see that by and large most of our indicator variables have a notable sense of correlatory attraction to active nests, with the exception of visible sightings and inactive nests (to an extent). If we had to choose a ‘most indicitive’ variable, we see the strongest attraction within tens of meters to an active nest with molted feathers, with whitewash, prey remains, and sighting + auditory combination quickly behind. All of these are present notable atteraction out to a roughly 500-1000 meter radius before transitioning to pure randomness. Goshawk call observation (auditory) are similarly attractive, just rising slower than the others. These results make a lot of sense, and match nicely with the visual presentation in our map. Feathers would be expected to be concentrated near the nest due to the amount of time they spend there. Furthermore, they are very protecteive of their nests both flying around and screaming at those who get too close, so it makes sense that they would be both seen + heard near nests. They also have a very distinctive call, making auditory identification easy. It’s important to remember, however, that this is still observational data so causation cannot be drawn.\n\n\nAs for our non-attractive variables, Inactive nests show a slight attraction at very short distances but quickly fade to just randomness, likely because the habitat is good for nesting but the past inhabitants have moved on. There is also limited data for this category, which makes testing harder. Unlike the other variables goshawk sightings seem to be largely random at all distances, perhaps due to large flight ranges. Some models peak out of the envelope into attraction for brief moments, but not enough for us to confidently see any correlation.\n\n\nWe spent a lot of our time in the analysis stage trying to figure out what bounds would most accurately represent the data, ultimately settling on the PACs. What we realized, though, is that despite the much more accurate location data, some of the PACs had little to no observations! After consulting a team member, we learned that there were some PACs that were not able to be visited due to factors including forest fire, presence of bears, lack of time, and covert drug operations. If we had more time to mess around with this data we would reach out to more study leaders and thoroughly comb through all the PACs to remove those that weren’t actually visited. This would even out our density plots and increase the accuracy of our K cross tests.\n\n\nBonus Density Test!\n\nDue to us Recieving the PAC we did want to at least try doing a density test, now that we are closer to the realm of properly bound data. There are still some questions of which PACs were surveyed, and how evenly they were surveyed, but due to time constraints and lack of communication to study leaders, we decided to just do an inhomogenous test, as this should account for some of those inequites. This test is mostly just a fun exploratory graph, and while we feel reasonably confident in the results, we would likely do much more to ensure its accuracy if we could.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs was largely expected, we see a (light, but notable) attraction within ~1000 meters distance, before it transitions to randomness - and then newly transtitions to a notable repelling ‘regular’ spacing after ~3000 meters. This makes a lot of sense, as while the observations will cluster with nests, due to the territorial nature of the birds the nests themselves will be spaced out, and the clustering observations should also follow this, causing a repelling trend at large distances– which we do in fact see here!\n\n\n\n\nAcknowledgements:\n\nWe would like to thank Dr. Boehm Vock for her patient expertise and assistance on our project. We would also like to thank Lynn Schofield, Visiting Intructor of Biology at St. Olaf and staff Biologist at the Institute for Bird Populations for providing data and answering our many questions.\nhttps://data.fs.usda.gov/geodata/edw/datasets.php?dsetCategory=boundaries\nAdditional Information from: https://www.allaboutbirds.org/news/the-basics-feather-molt/\nhttps://www.allaboutbirds.org/guide/American_Goshawk/overview"
  },
  {
    "objectID": "MP2.html",
    "href": "MP2.html",
    "title": "MP2",
    "section": "",
    "text": "After doing our amazing research on salmon we wanted to look at more data from Washington, and one thing that is becoming very prevalent is wildfires. Both of us have been in very close proximity to wildfires and have seen the aftermath of many of them. We want to look at the amount of forest land that was lost or affected by forest fires. Thinking through this data we quickly found a data set of the land area affected by fires, we then realized that it would be interesting to see the amount of forested area that was affected by the fires and then also the total land area that was affected in the county as a whole."
  },
  {
    "objectID": "MP2.html#motivations-behind-our-data",
    "href": "MP2.html#motivations-behind-our-data",
    "title": "MP2",
    "section": "",
    "text": "After doing our amazing research on salmon we wanted to look at more data from Washington, and one thing that is becoming very prevalent is wildfires. Both of us have been in very close proximity to wildfires and have seen the aftermath of many of them. We want to look at the amount of forest land that was lost or affected by forest fires. Thinking through this data we quickly found a data set of the land area affected by fires, we then realized that it would be interesting to see the amount of forested area that was affected by the fires and then also the total land area that was affected in the county as a whole."
  },
  {
    "objectID": "MP2.html#research-questions",
    "href": "MP2.html#research-questions",
    "title": "MP2",
    "section": "Research Questions",
    "text": "Research Questions\nWe are interested in looking at the total land area that is affected by forest fires in our home state. We wanted to not only look at the forest land that was affected by also the amount of land area that is forested and then also affected by the fire.\n\n# Our first table came from wikipedia, which is an allowed source\nis_valid_robotstxt(\"https://en.wikipedia.org/wiki/List_of_Washington_wildfires\")\n\n[1] TRUE\n\n#reading the html of the website\nwildfires &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_Washington_wildfires\")\n\n#scraping the table\nwildfiretables &lt;- html_nodes(wildfires, css = \"table\") \n\n#our first raw set of tables\nhtml_table(wildfiretables, header = TRUE, fill = TRUE)\n\n[[1]]\n# A tibble: 0 × 2\n# ℹ 2 variables:  &lt;lgl&gt;,\n#   This list is incomplete; you can help by adding missing items.  (August 2015) &lt;lgl&gt;\n\n[[2]]\n# A tibble: 11 × 11\n    Year `Fire name`       `Complex name` County `Start dateCause` `Size(acres)`\n   &lt;int&gt; &lt;chr&gt;             &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt;             &lt;chr&gt;        \n 1  2024 Beam Road Fire[2] \"\"             Yakima \"June 15\"         8,542 acres …\n 2  2024 Big Horn Fire[3]… \"\"             Klick… \"July 22, unknow… 51,569 acres…\n 3  2024 Black Canyon Fir… \"\"             Yakima \"July 22, unknow… 9,211 acres …\n 4  2024 Cougar Creek Fir… \"\"             Asoti… \"July 15, unknow… 20,699 acres…\n 5  2024 Pioneer Fire[8]   \"\"             Chelan \"June 8, human c… 36,763 acres…\n 6  2024 Retreat Fire[9][… \"\"             Yakima \"July 23, cause … 44,588 acres…\n 7  2024 Swawilla Fire[11… \"\"             Ferry… \"July 17, Lightn… 53,462 acres…\n 8  2023 Oregon Fire[13]   \"\"             Spoka… \"\"                10,817 acres…\n 9  2023 Gray Fire[15]     \"\"             Spoka… \"\"                10,085[15][1…\n10  2020 Cold Springs Can… \"Labor Day fi… Okano… \"\"                Over 410,000…\n11  2020 Whitney Fire      \"\"             Linco… \"September 7\"     127,430      \n# ℹ 5 more variables: Structureslost &lt;chr&gt;, Deaths &lt;chr&gt;, Injuries &lt;int&gt;,\n#   Notes &lt;chr&gt;, Image &lt;chr&gt;\n\n[[3]]\n# A tibble: 66 × 11\n    Year `Fire name`           `Complex name`  County `Start date` `Size(acres)`\n   &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;        \n 1  2019 243 Command Fire[18]  \"\"              Grant  \"June 3\"     20,380 acres…\n 2  2019 Cold Creek Fire[19]   \"\"              Benton \"\"           42,000 acres…\n 3  2019 Pipeline Fire         \"\"              Kitti… \"\"           6,515 acres …\n 4  2019 Powerline Fire[20]    \"\"              Grant  \"\"           7,800 acres …\n 5  2019 Williams Flats Fire   \"\"              Okano… \"\"           44,446 acres…\n 6  2016 Hart Fire             \"\"              Linco… \"\"           18,220       \n 7  2016 Range 12 Fire[21]     \"\"              Yakima \"\"           177,210      \n 8  2016 2016 Snake River Fire \"\"              Garfi… \"\"           11,452 acres…\n 9  2016 Spokane Complex Fire  \"Spokane Compl… Spoka… \"\"           7,251 acres …\n10  2015 Black Canyon Fire[22] \"Chelan Comple… Chelan \"August 14\"  6,761        \n# ℹ 56 more rows\n# ℹ 5 more variables: Structureslost &lt;chr&gt;, Deaths &lt;int&gt;, Injuries &lt;int&gt;,\n#   Notes &lt;chr&gt;, Image &lt;chr&gt;\n\n[[4]]\n# A tibble: 55 × 11\n    Year `Fire name`            `Complex name` County `Start date` `Size(acres)`\n   &lt;int&gt; &lt;chr&gt;                  &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;        \n 1  2009 Dry Creek Complex[50]  \"Dry Creek Co… Bento… \"\"           48,902       \n 2  2009 Oden Road Fire[50]     \"\"             Okano… \"\"           9,607        \n 3  2008 Badger Mountain Fire[… \"\"             Chela… \"\"           15,023       \n 4  2008 Cold Springs Fire      \"\"             Klick… \"\"           7,729        \n 5  2008 Columbia River Road F… \"\"             Okano… \"\"           22,115       \n 6  2008 Smith Lake Fire[64]    \"\"             Dougl… \"\"           12,513       \n 7  2008 Spokane Valley Fire[6… \"\"             Spoka… \"\"           1,008        \n 8  2008 Swanson Lake Fire[50]  \"\"             Linco… \"\"           19,090       \n 9  2007 Domke Lake Fire[50]    \"\"             Okano… \"\"           11,900       \n10  2007 Easy Street Fire[50]   \"\"             Chelan \"\"           5,209        \n# ℹ 45 more rows\n# ℹ 5 more variables: Structureslost &lt;int&gt;, Deaths &lt;int&gt;, Injuries &lt;chr&gt;,\n#   Notes &lt;chr&gt;, Image &lt;chr&gt;\n\n[[5]]\n# A tibble: 28 × 11\n    Year `Fire name`            `Complex name` County `Start date` `Size(acres)`\n   &lt;int&gt; &lt;chr&gt;                  &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;        \n 1  1998 Cleveland Fire[84]     \"\"             Klick… \"\"           18,500       \n 2  1998 Rattle Snake Ridge Fi… \"\"             Yakima \"\"           18,000       \n 3  1997 Olympia Command Fire[… \"\"             Benton \"\"           5,500        \n 4  1997 Pow Wah Kee Fire[1]    \"August 3\"     Asotin \"\"           8,000        \n 5  1996 Baird Springs Fire[1]  \"\"             Grant  \"August 2\"   14,000       \n 6  1996 Cold Creek Fire[50]    \"\"             Bento… \"\"           57,000       \n 7  1994 Copper Butte Fire[96]  \"\"             Ferry  \"\"           10,473       \n 8  1994 Rat Creek / Hatchery … \"\"             Chelan \"\"           43,000       \n 9  1994 Tyee Creek Fire[98][9… \"\"             Chelan \"\"           135,000      \n10  1992 Castlerock Fire[1]     \"\"             Wenat… \"\"           3,500[100]   \n# ℹ 18 more rows\n# ℹ 5 more variables: Structureslost &lt;chr&gt;, Deaths &lt;chr&gt;, Injuries &lt;chr&gt;,\n#   Notes &lt;chr&gt;, Image &lt;chr&gt;\n\n[[6]]\n# A tibble: 39 × 10\n    Year `Fire name`            `Complex name` County `Start date` `Size(acres)`\n   &lt;int&gt; &lt;chr&gt;                  &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;        \n 1  2024 Bridge Creek Fire      \"\"             Ferry  \"July 19\"    3,998 acres …\n 2  2016 Buck Creek             \"\"             Chelan \"July 22\"    1,987 acres …\n 3  2015 231 Fire               \"\"             Steve… \"\"           1,138        \n 4  2015 Twenty-One Mile Grade… \"\"             Ferry  \"\"           2,250        \n 5  2014 Hansel Fire            \"\"             Chelan \"\"           1,016        \n 6  2014 Little Bridge Fire     \"\"             Okano… \"August 2\"   4,896        \n 7  2014 Lone Mountain Fire     \"\"             Chelan \"July 14\"    2,770        \n 8  2012 Cashmere Fire          \"Wenatchee Co… Chelan \"\"           2,651        \n 9  2012 Highway 141 Fire[84]   \"\"             Klick… \"\"           1,644        \n10  2011 Salmon Fire[50]        \"\"             Okano… \"\"           1,631        \n# ℹ 29 more rows\n# ℹ 4 more variables: Structureslost &lt;int&gt;, Injuries &lt;int&gt;, Notes &lt;chr&gt;,\n#   Image &lt;chr&gt;\n\n[[7]]\n# A tibble: 0 × 2\n# ℹ 2 variables:  &lt;lgl&gt;,\n#   This list is incomplete; you can help by adding missing items.  (September 2015) &lt;lgl&gt;\n\n[[8]]\n# A tibble: 24 × 10\n   ``     Totalfires `Total area burned` `Total area burned` Structureslost  \n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;               &lt;chr&gt;               &lt;chr&gt;           \n 1 \"\"     Totalfires Acres               Hectares            \"Structureslost\"\n 2 \"2002\" 1,285      92,742              37,531              \"\"              \n 3 \"2003\" 1,373      200,517             81,146              \"\"              \n 4 \"2004\" 1,674      92,617              37,481              \"\"              \n 5 \"2005\" 998        185,748             75,170              \"\"              \n 6 \"2006\" 1,579      410,060             165,950             \"\"              \n 7 \"2007\" 1,268      214,925             86,977              \"\"              \n 8 \"2008\" 1,303      147,264             59,596              \"\"              \n 9 \"2009\" 1,976      77,250              31,260              \"\"              \n10 \"2010\" 870        56,820              22,990              \"\"              \n# ℹ 14 more rows\n# ℹ 5 more variables: Fatalities &lt;chr&gt;, Injuries &lt;chr&gt;, Totalcost &lt;chr&gt;,\n#   Notes &lt;chr&gt;, Source &lt;chr&gt;\n\n[[9]]\n# A tibble: 12 × 2\n   .mw-parser-output .navbar{display:inline;font-size:8…¹ .mw-parser-output .n…²\n   &lt;chr&gt;                                                  &lt;chr&gt;                 \n 1 \"Pre-2014\"                                             \"Yacolt Burn (1902)\\n…\n 2 \"2014\"                                                 \"Carlton Complex\"     \n 3 \"2015\"                                                 \"Okanogan Complex\"    \n 4 \"2016\"                                                 \"Range 12\"            \n 5 \"2017\"                                                 \"Diamond Creek\\nJack …\n 6 \"2018\"                                                 \"Soap Lake\\nMaple Fir…\n 7 \"2019\"                                                 \"243 Command Fire\\nLe…\n 8 \"2020\"                                                 \"Evans Canyon\\nLabor …\n 9 \"2021\"                                                 \"Schneider Springs Fi…\n10 \"2023\"                                                 \"Eagle Bluff Fire\\nGr…\n11 \"2024\"                                                 \"Pioneer Fire\\nRetrea…\n12 \"Category\\n Commons\"                                   \"Category\\n Commons\"  \n# ℹ abbreviated names:\n#   ¹​`.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a&gt;span,.mw-parser-output .navbar a&gt;abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}vteWashington wildfires`,\n#   ²​`.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a&gt;span,.mw-parser-output .navbar a&gt;abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}vteWashington wildfires`\n\n[[10]]\n# A tibble: 3 × 2\n  `vteWildfires in the United States` `vteWildfires in the United States`       \n  &lt;chr&gt;                               &lt;chr&gt;                                     \n1 \"States\"                            \"Alabama\\nAlaska\\nArizona\\nArkansas\\nCali…\n2 \"Territories\"                       \"American Samoa\\nGuam\\nNorthern Mariana I…\n3 \"Category\\n Commons\"                \"Category\\n Commons\"                      \n\n\n\n# Since we had so many tables from one scrape to use, we created a small \n# function to choose the table from the list using its subset number, cleaned \n# the names, remove unnecessary columns, and rename a common variables. Due to \n# inconsistency, all variables were set set as character and then parsed for \n# numbers.\n\ncleaninggg &lt;- function(table, i) {\n  html_table(table, header = TRUE, fill = TRUE)[[i]]|&gt; \n    janitor::clean_names() |&gt;\n    select(-notes, -image, -injuries, -complex_name) |&gt;\n    mutate(across(c(structureslost, size_acres), as.character),\n           across(c(structureslost, size_acres), parse_number)) |&gt;\n    rename(\"fire_size_acres\" = \"size_acres\")\n}\n\n# Running the function for each of the times to \n# pull the data out of the list from wikipedia into 5 (nearly) uniform datasets\ntwenty &lt;- cleaninggg(wildfiretables, 2) |&gt; rename(\"start_date\" = \"start_date_cause\")\nten &lt;- cleaninggg(wildfiretables, 3) \nthousand &lt;- cleaninggg(wildfiretables, 4)\nnines &lt;- cleaninggg(wildfiretables, 5) \nminors &lt;- cleaninggg(wildfiretables, 6) \n\n# Binds all of the major fires into one dataset and removes deaths for \n# consistency with the minor fires\nmajors &lt;- rbind(twenty, ten, thousand, nines) |&gt; select(-deaths)\n\n# Adds a column that identifies is a fire was major or minor\nminors['fire_type'] = \"Minor\"\nmajors['fire_type'] = \"Major\"\n\n# Joins all fires together\nfires &lt;- rbind(majors, minors)\n\nhead(fires)\n\n# A tibble: 6 × 7\n   year fire_name     county start_date fire_size_acres structureslost fire_type\n  &lt;int&gt; &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;    \n1  2024 Beam Road Fi… Yakima June 15               8542              0 Major    \n2  2024 Big Horn Fir… Klick… July 22, …           51569              0 Major    \n3  2024 Black Canyon… Yakima July 22, …            9211              0 Major    \n4  2024 Cougar Creek… Asoti… July 15, …           20699              4 Major    \n5  2024 Pioneer Fire… Chelan June 8, h…           36763              0 Major    \n6  2024 Retreat Fire… Yakima July 23, …           44588              5 Major    \n\n\n\n# As most major fires burn throughout forests, we wanted to add in a dataset \n# about forest coverage per county, we were planning to make a for-loop for \n# this, but all of the websites we tried to scrape weren't reading the actual \n# number as it was stored as an image? So we found this website that stores it\n# all as a list\nis_valid_robotstxt(\"https://data.workingforests.org/#\")\n\n[1] TRUE\n\nsession &lt;- bow(\"https://data.workingforests.org/#\")\n\n# Scraped the county names as one list\ncounty_title &lt;- scrape(session) |&gt;\n  html_nodes(\".countyName\") |&gt;\n  html_text()\n\nNo encoding supplied: defaulting to UTF-8.\n\ncounty_title\n\n [1] \"Statewide\"           \"Adams County\"        \"Asotin County\"      \n [4] \"Benton County\"       \"Chelan County\"       \"Clallam County\"     \n [7] \"Clark County\"        \"Columbia County\"     \"Cowlitz County\"     \n[10] \"Douglas County\"      \"Ferry County\"        \"Franklin County\"    \n[13] \"Garfield County\"     \"Grant County\"        \"Grays Harbor County\"\n[16] \"Island County\"       \"Jefferson County\"    \"King County\"        \n[19] \"Kitsap County\"       \"Kittitas County\"     \"Klickitat County\"   \n[22] \"Lewis County\"        \"Lincoln County\"      \"Mason County\"       \n[25] \"Okanogan County\"     \"Pacific County\"      \"Pend Oreille County\"\n[28] \"Pierce County\"       \"San Juan County\"     \"Skagit County\"      \n[31] \"Skamania County\"     \"Snohomish County\"    \"Spokane County\"     \n[34] \"Stevens County\"      \"Thurston County\"     \"Wahkiakum County\"   \n[37] \"Walla Walla County\"  \"Whatcom County\"      \"Whitman County\"     \n[40] \"Yakima County\"      \n\n# Scraped the forest coverage as another list\nforest_cov &lt;- scrape(session) |&gt;\n  html_nodes(\".dataValueEmphasized\") |&gt;\n  html_text()\nforest_cov\n\n [1] \"22,983,438\" \"1,452\"      \"103,022\"    \"351\"        \"1,392,891\" \n [6] \"1,034,606\"  \"251,273\"    \"203,917\"    \"657,909\"    \"16,983\"    \n[11] \"1,072,722\"  \"1,733\"      \"100,933\"    \"6,706\"      \"1,120,182\" \n[16] \"86,883\"     \"1,064,350\"  \"1,003,402\"  \"187,620\"    \"783,309\"   \n[21] \"516,397\"    \"1,374,647\"  \"69,114\"     \"552,926\"    \"1,982,401\" \n[26] \"534,690\"    \"787,506\"    \"800,881\"    \"85,258\"     \"890,416\"   \n[31] \"996,021\"    \"1,065,150\"  \"318,506\"    \"1,149,289\"  \"329,638\"   \n[36] \"147,694\"    \"30,934\"     \"1,033,817\"  \"26,889\"     \"1,201,021\" \n\n# Brought the 2 lists together as one tibble with 2 columns, removed \" County\"\n# from name to synchronize with main table\nforest_cover &lt;- tibble(county = county_title, \n                    forest_coverage_acres = forest_cov) |&gt;\n  mutate(county = str_remove(county, \" County\"),\n         forest_coverage_acres = parse_number(forest_coverage_acres))\n\n# Joins this forest coverage with our fire data by county. For ease of analysis\n# at this stage without knowing string analysis in detail (yet!), we removed all\n# rows that contained 2 counties by dropping NA's in forest coverage. This way \n# all rows should have a complete collection of county name, forest size, and \n# fire size. \nfullfires &lt;- fires |&gt; left_join(forest_cover) |&gt;\n  drop_na(forest_coverage_acres) \n\nJoining with `by = join_by(county)`\n\nhead(fullfires)\n\n# A tibble: 6 × 8\n   year fire_name     county start_date fire_size_acres structureslost fire_type\n  &lt;int&gt; &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;    \n1  2024 Beam Road Fi… Yakima \"June 15\"             8542              0 Major    \n2  2024 Big Horn Fir… Klick… \"July 22,…           51569              0 Major    \n3  2024 Black Canyon… Yakima \"July 22,…            9211              0 Major    \n4  2024 Pioneer Fire… Chelan \"June 8, …           36763              0 Major    \n5  2024 Retreat Fire… Yakima \"July 23,…           44588              5 Major    \n6  2023 Gray Fire[15] Spoka… \"\"                   10085            259 Major    \n# ℹ 1 more variable: forest_coverage_acres &lt;dbl&gt;\n\n\n\n# Lastly, we also thought it would be good to include the size of the counties \n# themselves as a comparison to the size of the forest its fires, so we scraped \n# this table\ncounties &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_counties_in_Washington\")\ncountytable &lt;- html_nodes(counties, css = \"table\") \ncountytable\n\n{xml_nodeset (8)}\n[1] &lt;table class=\"infobox vevent\" style=\"float: right; width: ;\"&gt;&lt;tbody&gt;\\n&lt;tr ...\n[2] &lt;table class=\"wikitable sortable sticky-header\" style=\"text-align: center ...\n[3] &lt;table class=\"nowraplinks mw-collapsible mw-collapsed navbox-inner\" style ...\n[4] &lt;table class=\"nowraplinks mw-collapsible autocollapse navbox-inner\" style ...\n[5] &lt;table class=\"nowraplinks hlist mw-collapsible autocollapse navbox-inner\" ...\n[6] &lt;table class=\"nowraplinks navbox-subgroup\" style=\"border-spacing:0\"&gt;&lt;tbod ...\n[7] &lt;table class=\"nowraplinks navbox-subgroup\" style=\"border-spacing:0\"&gt;&lt;tbod ...\n[8] &lt;table class=\"nowraplinks navbox-subgroup\" style=\"border-spacing:0\"&gt;&lt;tbod ...\n\n# This identifies the table we want, cleans the names, removes part of the name\n# ' County' for consistency, parses the sq. mi. and converts it to acres, and\n# selects just county and county size\n\ncountysize &lt;- html_table(countytable, header = TRUE, fill = TRUE)[[2]] |&gt; \n  janitor::clean_names() |&gt;\n  mutate(county = str_remove(county, \" County\"),\n         county_size_acres = parse_number(land_area_11) * 640) |&gt;\n    select(county, county_size_acres)\n\n# Finally ! We join this last table with the main dataset\nfinal_fires &lt;- fullfires |&gt; left_join(countysize)\n\nJoining with `by = join_by(county)`\n\nfinal_fires\n\n# A tibble: 170 × 9\n    year fire_name    county start_date fire_size_acres structureslost fire_type\n   &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;    \n 1  2024 Beam Road F… Yakima \"June 15\"             8542              0 Major    \n 2  2024 Big Horn Fi… Klick… \"July 22,…           51569              0 Major    \n 3  2024 Black Canyo… Yakima \"July 22,…            9211              0 Major    \n 4  2024 Pioneer Fir… Chelan \"June 8, …           36763              0 Major    \n 5  2024 Retreat Fir… Yakima \"July 23,…           44588              5 Major    \n 6  2023 Gray Fire[1… Spoka… \"\"                   10085            259 Major    \n 7  2020 Whitney Fire Linco… \"Septembe…          127430             NA Major    \n 8  2019 243 Command… Grant  \"June 3\"             20380              0 Major    \n 9  2019 Cold Creek … Benton \"\"                   42000             NA Major    \n10  2019 Pipeline Fi… Kitti… \"\"                    6515             NA Major    \n# ℹ 160 more rows\n# ℹ 2 more variables: forest_coverage_acres &lt;dbl&gt;, county_size_acres &lt;dbl&gt;\n\nhead(final_fires)\n\n# A tibble: 6 × 9\n   year fire_name     county start_date fire_size_acres structureslost fire_type\n  &lt;int&gt; &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;    \n1  2024 Beam Road Fi… Yakima \"June 15\"             8542              0 Major    \n2  2024 Big Horn Fir… Klick… \"July 22,…           51569              0 Major    \n3  2024 Black Canyon… Yakima \"July 22,…            9211              0 Major    \n4  2024 Pioneer Fire… Chelan \"June 8, …           36763              0 Major    \n5  2024 Retreat Fire… Yakima \"July 23,…           44588              5 Major    \n6  2023 Gray Fire[15] Spoka… \"\"                   10085            259 Major    \n# ℹ 2 more variables: forest_coverage_acres &lt;dbl&gt;, county_size_acres &lt;dbl&gt;"
  },
  {
    "objectID": "MP2.html#future-uses-of-this-data",
    "href": "MP2.html#future-uses-of-this-data",
    "title": "MP2",
    "section": "Future Uses of this Data",
    "text": "Future Uses of this Data\nFor future uses of this data we have a lot of things that we want to clean with string functions. We were also looking into census data for each county in Washington, which would be interesting to see if there is higher population in a county that has more forest fire activity. It would also be interesting to add spatial data to this to map the percentage of forest area affected by fires or other percentage maps."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Aria!",
    "section": "",
    "text": "My Major + Concentrations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI’m a biology major, with a specific interest in marine ecology, and a growing love for birds!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI also concentrate in environmental studies, where I am passionate about engaging with prescribed burns + regenerative farming, as well as Statistics & Data science, where I especially love spatial statistics and mapping!\n\n\n\n\nMy Jobs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI currently have 4 jobs on campus: in the ceramic studio, at Stogrow, as an RA, and as an R-Star!\n\n\n\n\nMy Personal Interests\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI love to cook and bake whenever I can, fun fact: I spent my gap-year as a head cook! I love to bake bread whenever I can :)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs a vegetarian I love to both eat and observe fungi whenever I can!\n\n\n\n\n\n\n\n\n\n\n\n\nLastly, snakes are my favorite animal!"
  },
  {
    "objectID": "MiniTextProject.html",
    "href": "MiniTextProject.html",
    "title": "Mini Text Project",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(jsonlite)\n\nWarning: package 'jsonlite' was built under R version 4.3.3\n\n\n\nAttaching package: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(tidytext)\nlibrary(textdata)\n\nWarning: package 'textdata' was built under R version 4.3.3\n\nlibrary(wordcloud)\n\nWarning: package 'wordcloud' was built under R version 4.3.3\n\n\nLoading required package: RColorBrewer\n\nlibrary(ggthemes)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\n\ndata &lt;- read_json(\"https://github.com/leinstay/steamdb/raw/refs/heads/main/steamdb.json\")\n\n\ndata2 &lt;- as.data.frame(do.call(rbind, data))\n \n \n data2 &lt;- data2 |&gt; separate_wider_delim(published_store, delim = \"-\",\n          names = c(\"publishedyear\", \"publishedmonth\", \"publishedday\"), too_few = \"debug\")\n\nWarning in stri_split_fixed(string, pattern, n = n, simplify = simplify, :\nargument is not an atomic vector; coercing\n\n\nWarning: Debug mode activated: adding variables `published_store_ok`,\n`published_store_pieces`, and `published_store_remainder`.\n\n\nWarning in stri_locate_all_fixed(string, pattern, omit_no_match = TRUE, :\nargument is not an atomic vector; coercing\n\n\nWarning in stri_sub(string, from = start, to = end): argument is not an atomic\nvector; coercing\n\n\n\n data2 |&gt; drop_na(publishedyear) |&gt; \n   filter(publishedyear &gt; 2006,\n          publishedyear != \"NULL\") |&gt;\n   mutate(sequel = str_detect(name, \"\\\\w* \\\\d\\\\d?\\\\b\")) |&gt; \n   group_by(publishedyear) |&gt; \n   summarize(\"Number of Sequels\" = sum(sequel),\n            \"Total Games\" = n(),\n            \"Proportion are Sequels\" = mean(sequel)) |&gt; print(n=100) |&gt;\n   kable(digits=3) |&gt;\n   kable_styling(bootstrap_options = \"striped\", full_width = FALSE,\n                 position = \"float_right\")\n\n# A tibble: 16 × 4\n   publishedyear `Number of Sequels` `Total Games` `Proportion are Sequels`\n   &lt;chr&gt;                       &lt;int&gt;         &lt;int&gt;                    &lt;dbl&gt;\n 1 2007                           17           106                   0.160 \n 2 2008                           25           167                   0.150 \n 3 2009                           52           332                   0.157 \n 4 2010                           36           280                   0.129 \n 5 2011                           35           281                   0.125 \n 6 2012                           45           342                   0.132 \n 7 2013                           57           475                   0.12  \n 8 2014                          167          1589                   0.105 \n 9 2015                          177          2635                   0.0672\n10 2016                          243          4312                   0.0564\n11 2017                          350          6232                   0.0562\n12 2018                          395          8169                   0.0484\n13 2019                          428          8014                   0.0534\n14 2020                          511          9636                   0.0530\n15 2021                          433          7886                   0.0549\n16 2022                          183          3283                   0.0557\n\n\n\n\n\npublishedyear\nNumber of Sequels\nTotal Games\nProportion are Sequels\n\n\n\n\n2007\n17\n106\n0.160\n\n\n2008\n25\n167\n0.150\n\n\n2009\n52\n332\n0.157\n\n\n2010\n36\n280\n0.129\n\n\n2011\n35\n281\n0.125\n\n\n2012\n45\n342\n0.132\n\n\n2013\n57\n475\n0.120\n\n\n2014\n167\n1589\n0.105\n\n\n2015\n177\n2635\n0.067\n\n\n2016\n243\n4312\n0.056\n\n\n2017\n350\n6232\n0.056\n\n\n2018\n395\n8169\n0.048\n\n\n2019\n428\n8014\n0.053\n\n\n2020\n511\n9636\n0.053\n\n\n2021\n433\n7886\n0.055\n\n\n2022\n183\n3283\n0.056\n\n\n\n\n data2 |&gt;\n  select(name, publishers) |&gt;\n  filter(str_detect(name, \"^(..\\\\w*) +.* *\\\\1+$\")) |&gt;\n  arrange(desc(publishers)) |&gt;\n  kable() |&gt;\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position = \"float_right\")\n\n\n\n\nname\npublishers\n\n\n\n\nBad Bad\n3000Studio\n\n\nOne By One\n玫瑰工作室 |\n\n\nRun Girls Run\n?\n\n\nArmy To Army\nPiece Of Voxel\n\n\nHunter A Hunter\nRagent Sdn Bhd\n\n\nWall to Wall\nbeats rolls\n\n\nStalks Stalks Stalks\nGlacial Lake Studios\n\n\nDay After Day\nKielek577\n\n\nDice vs Dice\nVulpis Yazilim\n\n\nRace - Total Toon Race\nAndreas Otten\n\n\nDay Repeat Day\nKimmo Factor Oy\n\n\nHome Designer - Home Sweet Home\nHH-Games\n\n\nWind Wind\nELIOT\n\n\nRun Ralph Run\nDnovel\n\n\nComplex Complex\nBlaze Epic\n\n\nDungeon No Dungeon\n302 Studio\n\n\nKeep the Keep\nNebulate.me\n\n\nLeHweng LeHweng\nsoniczwa\n\n\nGurney to Gurney\nLucky Games\n\n\nVR Zeppelin Airship Trips: Flying hotel experiences in VR\nWilliam at Oxford\n\n\nTruck Truck\nTruck Truck hede\n\n\nAugust of August\nFireworks Garland\n\n\nRun Dummy Run\nImperiumGame\n\n\nTrain Train Train\nheinn\n\n\nWar War\nVerto Studio LLC\n\n\nFly You Fly\nIndiedog Games\n\n\nWay of Boy: Another Way\nFDRAGONS\n\n\nBatu Ta Batu\n2Awesome Studio\n\n\nSpace Raiders in Space\nDestructive Creations\n\n\nBock Bock\nBen Fitzpatrick\n\n\nTime To Stop Time\nCreativeForge Games\n\n\nPaw Paw Paw\nGrabTheGames\n\n\nNeko Neko\nNeko Game\n\n\nRUN HARE RUN\nkhukhrovr\n\n\nSpace Space\nGino Di Pierro\n\n\nCreme de la Creme\nChoice of Games\n\n\nBrother Brother\nNone\n\n\nYUT YUT\nBenjamin Nielsen\n\n\nMi Mi Mi\nIR Studio\n\n\nBoon Boon\nAnamik Majumdar\n\n\nlove love love\nTintin Game\n\n\nPiko Piko\nDegica\n\n\nMore and More\nProject_lelz\n\n\nPrincess Maker Go!Go! Princess\nCFK Co., Ltd.\n\n\nMonster X Monster\nVanGame\n\n\nMoney Makes Money\nSpaghetti Code Ltd\n\n\nSpace Rabbits in Space\nEuropean Games Group AG\n\n\nSnake vs Snake\nCasualGames.nu\n\n\nEleven Eleven\nUniversal Studios Interactive Entertainment LLC\n\n\nInch by Inch\nDare Looks\n\n\nRun Naked Woman Run\nTero Lunkka\n\n\nSwap Swap\nMediaPod\n\n\nSecond Second\nSinkhole Studio\n\n\nJuan v Juan\nTwo Taters\n\n\nNEO NEO\nKastrye\n\n\nUp And Up\nJunitre Works\n\n\nDefense Task Force - Sci Fi Tower Defense\nChillX Ltd\n\n\nFloor By Floor\nAA Games\n\n\nBump Bump Bump\nGolden Frog\n\n\nRun Dorothy Run\nVirtro Entertainment\n\n\nSos i Pie Sos\nGame for people\n\n\nRun Zeus Run\nGamerzDan\n\n\nTrivia Vault: Mixed Trivia\nRipknot Systems\n\n\nTrivia Vault: Science & History Trivia\nRipknot Systems\n\n\nTrivia Vault: Super Heroes Trivia\nRipknot Systems\n\n\nTrivia Vault: 1980's Trivia\nRipknot Systems\n\n\nTrivia Vault: Classic Rock Trivia\nRipknot Systems\n\n\nTrivia Vault: Mini Mixed Trivia\nRipknot Systems\n\n\nTrivia Vault Football Trivia\nRipknot Systems\n\n\nTrivia Vault Baseball Trivia\nRipknot Systems\n\n\nTrivia Vault Olympics Trivia\nRipknot Systems\n\n\nTrivia Vault Basketball Trivia\nRipknot Systems\n\n\nTrivia Vault: Boxing Trivia\nRipknot Systems\n\n\nTrivia Vault: Auto Racing Trivia\nRipknot Systems\n\n\nTrivia Vault: Golf Trivia\nRipknot Systems\n\n\nTrivia Vault: Hockey Trivia\nRipknot Systems\n\n\nTrivia Vault: Tennis Trivia\nRipknot Systems\n\n\nTrivia Vault: Soccer Trivia\nRipknot Systems\n\n\nTrivia Vault: Movie Trivia\nRipknot Systems\n\n\nTrivia Vault: Celebrity Trivia\nRipknot Systems\n\n\nTrivia Vault: TV Trivia\nRipknot Systems\n\n\nTrivia Vault: Art Trivia\nRipknot Systems\n\n\nTrivia Vault: Business Trivia\nRipknot Systems\n\n\nTrivia Vault: Toy Trivia\nRipknot Systems\n\n\nTrivia Vault: Fashion Trivia\nRipknot Systems\n\n\nTrivia Vault: Food Trivia\nRipknot Systems\n\n\nTrivia Vault: Literature Trivia\nRipknot Systems\n\n\nTrivia Vault: Music Trivia\nRipknot Systems\n\n\nSakura Sakura\nSol Press\n\n\nSquare x Square\nPirates Of Love\n\n\nHome Sweet Home\nYGGDRAZIL GROUP CO.,LTD\n\n\nVolleyball Unbound - Pro Beach Volleyball\nGreat Boolean\n\n\nSlap Village: Reality Slap\nMonkeyToons\n\n\nTime in Time\nErayTek\n\n\nCrush Crush\nSad Panda Studios\n\n\nBlush Blush\nSad Panda Studios\n\n\nParty Saboteurs: After Party\nThe Glitch Factory\n\n\nVR Karts SteamVR\nViewpoint Games\n\n\nDoor To Door\nCD Jones\n\n\nReverse x Reverse\nSekai Project\n\n\nChalo Chalo\nRedshift Media,Sparpweed\n\n\nRun Rabbit Run\nAbsolutist Ltd.\n\n\nBeat Da Beat\nNekki Limited\n\n\nAtlantis 2: Beyond Atlantis\nMicroids\n\n\nAvalanche 2: Super Avalanche\nMidnight City\n\n\nMillennium 5 - The Battle of the Millennium\nAldorlea Games\n\n\nSneaky Sneaky\nNaiad Entertainment LLC\n\n\n\n\n data2 |&gt; select(name, full_price, meta_score) |&gt; \n   mutate(words_in_name = str_count(name,\"\\\\b[^ ]+\\\\b\"),\n          full_price = str_replace(full_price, \"NULL\", \"0\"),\n          full_price = as.numeric(full_price),\n          full_price = full_price/100) |&gt;\n   filter(full_price &lt; 300) |&gt;\n   ggplot(aes(x = words_in_name, y = full_price)) + geom_point(alpha = .1) +\n   geom_smooth(method = lm) +\n   theme_clean()\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `full_price = str_replace(full_price, \"NULL\", \"0\")`.\nCaused by warning in `stri_replace_first_regex()`:\n! argument is not an atomic vector; coercing\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\npricedata &lt;- data2 |&gt; select(name, full_price) |&gt; \n   mutate(words_in_name = str_count(name,\"\\\\b[^ ]+\\\\b\"),\n          full_price = str_replace(full_price, \"NULL\", \"0\"),\n          full_price = as.numeric(full_price),\n          full_price = full_price/100) |&gt;\n   filter(full_price &lt; 300)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `full_price = str_replace(full_price, \"NULL\", \"0\")`.\nCaused by warning in `stri_replace_first_regex()`:\n! argument is not an atomic vector; coercing\n\nsummary(lm(full_price ~ words_in_name, data = pricedata))\n\n\nCall:\nlm(formula = full_price ~ words_in_name, data = pricedata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.297  -5.947  -2.350   2.650 193.247 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    6.14548    0.07965   77.16   &lt;2e-16 ***\nwords_in_name  0.59732    0.02408   24.80   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.435 on 53977 degrees of freedom\nMultiple R-squared:  0.01127,   Adjusted R-squared:  0.01125 \nF-statistic: 615.2 on 1 and 53977 DF,  p-value: &lt; 2.2e-16\n\n data2 |&gt; select(languages, publishedyear, full_price) |&gt;\n   mutate(num_commas = str_count(languages,\",\"),\n          num_languages = num_commas + 1,\n          publishedyear = as.numeric(publishedyear),\n          full_price = str_replace(full_price, \"NULL\", \"0\"),\n          full_price = as.numeric(full_price),\n          full_price = full_price/100) |&gt;\n   filter(full_price == 59.99) |&gt;\n   group_by(publishedyear) |&gt;\n   summarize(avg_language = mean(num_languages)) |&gt;\n   filter(publishedyear &gt; 2006) |&gt;\n   kable(digits = 1) |&gt;\n   kable_styling(bootstrap_options = \"striped\", full_width = FALSE,\n                 position = \"float_right\")\n\nWarning: There were 3 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `num_commas = str_count(languages, \",\")`.\nCaused by warning in `stri_count_regex()`:\n! argument is not an atomic vector; coercing\nℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.\n\n\n\n\n\npublishedyear\navg_language\n\n\n\n\n2012\n5.0\n\n\n2013\n9.0\n\n\n2014\n3.2\n\n\n2015\n3.8\n\n\n2016\n7.8\n\n\n2017\n6.7\n\n\n2018\n8.4\n\n\n2019\n8.4\n\n\n2020\n8.3\n\n\n2021\n8.3\n\n\n2022\n11.5\n\n\n\n\n  data2 |&gt; \n   mutate(words_in_name = str_count(name,\"\\\\b[^ ]+\\\\b\"),\n          num_commas = str_count(languages,\",\"),\n          num_languages = num_commas + 1,\n          publishedyear = as.numeric(publishedyear),\n          full_price = str_replace(full_price, \"NULL\", \"0\"),\n          full_price = as.numeric(full_price),\n          full_price = full_price/100) |&gt;\n   group_by(publishedyear) |&gt;\n   summarize(avg_words = mean(words_in_name)) |&gt;\n   filter(publishedyear &gt; 2006) |&gt;\n   kable(digits = 1) |&gt;\n   kable_styling(bootstrap_options = \"striped\", full_width = FALSE,\n                 position = \"float_right\")\n\nWarning: There were 3 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `num_commas = str_count(languages, \",\")`.\nCaused by warning in `stri_count_regex()`:\n! argument is not an atomic vector; coercing\nℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.\n\n\n\n\n\npublishedyear\navg_words\n\n\n\n\n2007\n3.8\n\n\n2008\n3.7\n\n\n2009\n3.7\n\n\n2010\n3.4\n\n\n2011\n3.2\n\n\n2012\n3.2\n\n\n2013\n3.2\n\n\n2014\n3.2\n\n\n2015\n3.0\n\n\n2016\n2.9\n\n\n2017\n2.9\n\n\n2018\n2.8\n\n\n2019\n2.8\n\n\n2020\n2.8\n\n\n2021\n2.8\n\n\n2022\n2.8\n\n\n\n\n\n\n#the json data waas very broken, not sure if these count as strings and regular expressions for my 3, but basically i'm just tricking r into turning into proper text\n descriptions &lt;- data2 |&gt; \n   mutate(names = str_extract(name, \".*\"),\n          descriptions = str_extract(description, \".*\")) |&gt;\n   select(descriptions, names, publishedyear)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `descriptions = str_extract(description, \".*\")`.\nCaused by warning in `stri_extract_first_regex()`:\n! argument is not an atomic vector; coercing\n\n tidy_descriptions &lt;- descriptions |&gt;\n  mutate(line = row_number()) |&gt;\n  unnest_tokens(word, descriptions, token = \"words\")\n   \nsmart_stopwords &lt;- get_stopwords(source = \"smart\")\n\nfont_stopwords &lt;- tibble(\n  word = c(\"br\",\"li\",\"strong\",\"ul\",\"quot\", \"game\"), \n  lexicon = \"font\")\n\n\n#top 20 words of all times\ntidy_descriptions |&gt;\n   filter(publishedyear != \"NULL\") |&gt;\n  anti_join(smart_stopwords) |&gt;\n  anti_join(font_stopwords) |&gt;\n  count(word, sort = TRUE) |&gt;\n  filter(word != \"NA\") |&gt;\n  slice_max(n, n = 20) |&gt;\n  ggplot(aes(fct_reorder(word, n), n)) +\n  geom_col() +\n  coord_flip()\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(word)`\n\n\n\n\n\n\n\n\n#top 10 word for each year (I tried so hard to order them for each year but it doesn't work )\ntidy_descriptions |&gt;\n   filter(publishedyear != \"NULL\",\n          publishedyear &gt; 2006) |&gt;\n  anti_join(smart_stopwords) |&gt;\n  anti_join(font_stopwords) |&gt;\n  group_by(publishedyear, word) |&gt;\n  summarize(n =n()) |&gt; \n  arrange(publishedyear, desc(n)) |&gt;\n  slice_max(n, n = 10) |&gt; \n  ggplot(aes(fct_reorder(word, n), n, fill = publishedyear)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ publishedyear, scales = \"free\")\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(word)`\n`summarise()` has grouped output by 'publishedyear'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\nwords &lt;- tidy_descriptions |&gt;\n  anti_join(stop_words) |&gt;\n  anti_join(font_stopwords) |&gt;\n  count(word) |&gt;\n  filter(word != \"NA\") |&gt;\n  arrange(desc(n))\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(word)`\n\n# word cloud of all years\nwordcloud(\n  words = words$word, \n  freq = words$n, \n  max.words = 200, \n  random.order = FALSE, \n  rot.per = 0.35,\n  scale = c(3.5, 0.25),\n  colors = brewer.pal(9, \"Dark2\"))\n\nWarning in brewer.pal(9, \"Dark2\"): n too large, allowed maximum for palette Dark2 is 8\nReturning the palette you asked for with that many colors"
  },
  {
    "objectID": "MiniTextProject.html#unifished-framework-of-mp4",
    "href": "MiniTextProject.html#unifished-framework-of-mp4",
    "title": "Mini Text Project",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(jsonlite)\n\nWarning: package 'jsonlite' was built under R version 4.3.3\n\n\n\nAttaching package: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(tidytext)\nlibrary(textdata)\n\nWarning: package 'textdata' was built under R version 4.3.3\n\nlibrary(wordcloud)\n\nWarning: package 'wordcloud' was built under R version 4.3.3\n\n\nLoading required package: RColorBrewer\n\nlibrary(ggthemes)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\n\ndata &lt;- read_json(\"https://github.com/leinstay/steamdb/raw/refs/heads/main/steamdb.json\")\n\n\ndata2 &lt;- as.data.frame(do.call(rbind, data))\n \n \n data2 &lt;- data2 |&gt; separate_wider_delim(published_store, delim = \"-\",\n          names = c(\"publishedyear\", \"publishedmonth\", \"publishedday\"), too_few = \"debug\")\n\nWarning in stri_split_fixed(string, pattern, n = n, simplify = simplify, :\nargument is not an atomic vector; coercing\n\n\nWarning: Debug mode activated: adding variables `published_store_ok`,\n`published_store_pieces`, and `published_store_remainder`.\n\n\nWarning in stri_locate_all_fixed(string, pattern, omit_no_match = TRUE, :\nargument is not an atomic vector; coercing\n\n\nWarning in stri_sub(string, from = start, to = end): argument is not an atomic\nvector; coercing\n\n\n\n data2 |&gt; drop_na(publishedyear) |&gt; \n   filter(publishedyear &gt; 2006,\n          publishedyear != \"NULL\") |&gt;\n   mutate(sequel = str_detect(name, \"\\\\w* \\\\d\\\\d?\\\\b\")) |&gt; \n   group_by(publishedyear) |&gt; \n   summarize(\"Number of Sequels\" = sum(sequel),\n            \"Total Games\" = n(),\n            \"Proportion are Sequels\" = mean(sequel)) |&gt; print(n=100) |&gt;\n   kable(digits=3) |&gt;\n   kable_styling(bootstrap_options = \"striped\", full_width = FALSE,\n                 position = \"float_right\")\n\n# A tibble: 16 × 4\n   publishedyear `Number of Sequels` `Total Games` `Proportion are Sequels`\n   &lt;chr&gt;                       &lt;int&gt;         &lt;int&gt;                    &lt;dbl&gt;\n 1 2007                           17           106                   0.160 \n 2 2008                           25           167                   0.150 \n 3 2009                           52           332                   0.157 \n 4 2010                           36           280                   0.129 \n 5 2011                           35           281                   0.125 \n 6 2012                           45           342                   0.132 \n 7 2013                           57           475                   0.12  \n 8 2014                          167          1589                   0.105 \n 9 2015                          177          2635                   0.0672\n10 2016                          243          4312                   0.0564\n11 2017                          350          6232                   0.0562\n12 2018                          395          8169                   0.0484\n13 2019                          428          8014                   0.0534\n14 2020                          511          9636                   0.0530\n15 2021                          433          7886                   0.0549\n16 2022                          183          3283                   0.0557\n\n\n\n\n\npublishedyear\nNumber of Sequels\nTotal Games\nProportion are Sequels\n\n\n\n\n2007\n17\n106\n0.160\n\n\n2008\n25\n167\n0.150\n\n\n2009\n52\n332\n0.157\n\n\n2010\n36\n280\n0.129\n\n\n2011\n35\n281\n0.125\n\n\n2012\n45\n342\n0.132\n\n\n2013\n57\n475\n0.120\n\n\n2014\n167\n1589\n0.105\n\n\n2015\n177\n2635\n0.067\n\n\n2016\n243\n4312\n0.056\n\n\n2017\n350\n6232\n0.056\n\n\n2018\n395\n8169\n0.048\n\n\n2019\n428\n8014\n0.053\n\n\n2020\n511\n9636\n0.053\n\n\n2021\n433\n7886\n0.055\n\n\n2022\n183\n3283\n0.056\n\n\n\n\n data2 |&gt;\n  select(name, publishers) |&gt;\n  filter(str_detect(name, \"^(..\\\\w*) +.* *\\\\1+$\")) |&gt;\n  arrange(desc(publishers)) |&gt;\n  kable() |&gt;\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position = \"float_right\")\n\n\n\n\nname\npublishers\n\n\n\n\nBad Bad\n3000Studio\n\n\nOne By One\n玫瑰工作室 |\n\n\nRun Girls Run\n?\n\n\nArmy To Army\nPiece Of Voxel\n\n\nHunter A Hunter\nRagent Sdn Bhd\n\n\nWall to Wall\nbeats rolls\n\n\nStalks Stalks Stalks\nGlacial Lake Studios\n\n\nDay After Day\nKielek577\n\n\nDice vs Dice\nVulpis Yazilim\n\n\nRace - Total Toon Race\nAndreas Otten\n\n\nDay Repeat Day\nKimmo Factor Oy\n\n\nHome Designer - Home Sweet Home\nHH-Games\n\n\nWind Wind\nELIOT\n\n\nRun Ralph Run\nDnovel\n\n\nComplex Complex\nBlaze Epic\n\n\nDungeon No Dungeon\n302 Studio\n\n\nKeep the Keep\nNebulate.me\n\n\nLeHweng LeHweng\nsoniczwa\n\n\nGurney to Gurney\nLucky Games\n\n\nVR Zeppelin Airship Trips: Flying hotel experiences in VR\nWilliam at Oxford\n\n\nTruck Truck\nTruck Truck hede\n\n\nAugust of August\nFireworks Garland\n\n\nRun Dummy Run\nImperiumGame\n\n\nTrain Train Train\nheinn\n\n\nWar War\nVerto Studio LLC\n\n\nFly You Fly\nIndiedog Games\n\n\nWay of Boy: Another Way\nFDRAGONS\n\n\nBatu Ta Batu\n2Awesome Studio\n\n\nSpace Raiders in Space\nDestructive Creations\n\n\nBock Bock\nBen Fitzpatrick\n\n\nTime To Stop Time\nCreativeForge Games\n\n\nPaw Paw Paw\nGrabTheGames\n\n\nNeko Neko\nNeko Game\n\n\nRUN HARE RUN\nkhukhrovr\n\n\nSpace Space\nGino Di Pierro\n\n\nCreme de la Creme\nChoice of Games\n\n\nBrother Brother\nNone\n\n\nYUT YUT\nBenjamin Nielsen\n\n\nMi Mi Mi\nIR Studio\n\n\nBoon Boon\nAnamik Majumdar\n\n\nlove love love\nTintin Game\n\n\nPiko Piko\nDegica\n\n\nMore and More\nProject_lelz\n\n\nPrincess Maker Go!Go! Princess\nCFK Co., Ltd.\n\n\nMonster X Monster\nVanGame\n\n\nMoney Makes Money\nSpaghetti Code Ltd\n\n\nSpace Rabbits in Space\nEuropean Games Group AG\n\n\nSnake vs Snake\nCasualGames.nu\n\n\nEleven Eleven\nUniversal Studios Interactive Entertainment LLC\n\n\nInch by Inch\nDare Looks\n\n\nRun Naked Woman Run\nTero Lunkka\n\n\nSwap Swap\nMediaPod\n\n\nSecond Second\nSinkhole Studio\n\n\nJuan v Juan\nTwo Taters\n\n\nNEO NEO\nKastrye\n\n\nUp And Up\nJunitre Works\n\n\nDefense Task Force - Sci Fi Tower Defense\nChillX Ltd\n\n\nFloor By Floor\nAA Games\n\n\nBump Bump Bump\nGolden Frog\n\n\nRun Dorothy Run\nVirtro Entertainment\n\n\nSos i Pie Sos\nGame for people\n\n\nRun Zeus Run\nGamerzDan\n\n\nTrivia Vault: Mixed Trivia\nRipknot Systems\n\n\nTrivia Vault: Science & History Trivia\nRipknot Systems\n\n\nTrivia Vault: Super Heroes Trivia\nRipknot Systems\n\n\nTrivia Vault: 1980's Trivia\nRipknot Systems\n\n\nTrivia Vault: Classic Rock Trivia\nRipknot Systems\n\n\nTrivia Vault: Mini Mixed Trivia\nRipknot Systems\n\n\nTrivia Vault Football Trivia\nRipknot Systems\n\n\nTrivia Vault Baseball Trivia\nRipknot Systems\n\n\nTrivia Vault Olympics Trivia\nRipknot Systems\n\n\nTrivia Vault Basketball Trivia\nRipknot Systems\n\n\nTrivia Vault: Boxing Trivia\nRipknot Systems\n\n\nTrivia Vault: Auto Racing Trivia\nRipknot Systems\n\n\nTrivia Vault: Golf Trivia\nRipknot Systems\n\n\nTrivia Vault: Hockey Trivia\nRipknot Systems\n\n\nTrivia Vault: Tennis Trivia\nRipknot Systems\n\n\nTrivia Vault: Soccer Trivia\nRipknot Systems\n\n\nTrivia Vault: Movie Trivia\nRipknot Systems\n\n\nTrivia Vault: Celebrity Trivia\nRipknot Systems\n\n\nTrivia Vault: TV Trivia\nRipknot Systems\n\n\nTrivia Vault: Art Trivia\nRipknot Systems\n\n\nTrivia Vault: Business Trivia\nRipknot Systems\n\n\nTrivia Vault: Toy Trivia\nRipknot Systems\n\n\nTrivia Vault: Fashion Trivia\nRipknot Systems\n\n\nTrivia Vault: Food Trivia\nRipknot Systems\n\n\nTrivia Vault: Literature Trivia\nRipknot Systems\n\n\nTrivia Vault: Music Trivia\nRipknot Systems\n\n\nSakura Sakura\nSol Press\n\n\nSquare x Square\nPirates Of Love\n\n\nHome Sweet Home\nYGGDRAZIL GROUP CO.,LTD\n\n\nVolleyball Unbound - Pro Beach Volleyball\nGreat Boolean\n\n\nSlap Village: Reality Slap\nMonkeyToons\n\n\nTime in Time\nErayTek\n\n\nCrush Crush\nSad Panda Studios\n\n\nBlush Blush\nSad Panda Studios\n\n\nParty Saboteurs: After Party\nThe Glitch Factory\n\n\nVR Karts SteamVR\nViewpoint Games\n\n\nDoor To Door\nCD Jones\n\n\nReverse x Reverse\nSekai Project\n\n\nChalo Chalo\nRedshift Media,Sparpweed\n\n\nRun Rabbit Run\nAbsolutist Ltd.\n\n\nBeat Da Beat\nNekki Limited\n\n\nAtlantis 2: Beyond Atlantis\nMicroids\n\n\nAvalanche 2: Super Avalanche\nMidnight City\n\n\nMillennium 5 - The Battle of the Millennium\nAldorlea Games\n\n\nSneaky Sneaky\nNaiad Entertainment LLC\n\n\n\n\n data2 |&gt; select(name, full_price, meta_score) |&gt; \n   mutate(words_in_name = str_count(name,\"\\\\b[^ ]+\\\\b\"),\n          full_price = str_replace(full_price, \"NULL\", \"0\"),\n          full_price = as.numeric(full_price),\n          full_price = full_price/100) |&gt;\n   filter(full_price &lt; 300) |&gt;\n   ggplot(aes(x = words_in_name, y = full_price)) + geom_point(alpha = .1) +\n   geom_smooth(method = lm) +\n   theme_clean()\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `full_price = str_replace(full_price, \"NULL\", \"0\")`.\nCaused by warning in `stri_replace_first_regex()`:\n! argument is not an atomic vector; coercing\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\npricedata &lt;- data2 |&gt; select(name, full_price) |&gt; \n   mutate(words_in_name = str_count(name,\"\\\\b[^ ]+\\\\b\"),\n          full_price = str_replace(full_price, \"NULL\", \"0\"),\n          full_price = as.numeric(full_price),\n          full_price = full_price/100) |&gt;\n   filter(full_price &lt; 300)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `full_price = str_replace(full_price, \"NULL\", \"0\")`.\nCaused by warning in `stri_replace_first_regex()`:\n! argument is not an atomic vector; coercing\n\nsummary(lm(full_price ~ words_in_name, data = pricedata))\n\n\nCall:\nlm(formula = full_price ~ words_in_name, data = pricedata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.297  -5.947  -2.350   2.650 193.247 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    6.14548    0.07965   77.16   &lt;2e-16 ***\nwords_in_name  0.59732    0.02408   24.80   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.435 on 53977 degrees of freedom\nMultiple R-squared:  0.01127,   Adjusted R-squared:  0.01125 \nF-statistic: 615.2 on 1 and 53977 DF,  p-value: &lt; 2.2e-16\n\n data2 |&gt; select(languages, publishedyear, full_price) |&gt;\n   mutate(num_commas = str_count(languages,\",\"),\n          num_languages = num_commas + 1,\n          publishedyear = as.numeric(publishedyear),\n          full_price = str_replace(full_price, \"NULL\", \"0\"),\n          full_price = as.numeric(full_price),\n          full_price = full_price/100) |&gt;\n   filter(full_price == 59.99) |&gt;\n   group_by(publishedyear) |&gt;\n   summarize(avg_language = mean(num_languages)) |&gt;\n   filter(publishedyear &gt; 2006) |&gt;\n   kable(digits = 1) |&gt;\n   kable_styling(bootstrap_options = \"striped\", full_width = FALSE,\n                 position = \"float_right\")\n\nWarning: There were 3 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `num_commas = str_count(languages, \",\")`.\nCaused by warning in `stri_count_regex()`:\n! argument is not an atomic vector; coercing\nℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.\n\n\n\n\n\npublishedyear\navg_language\n\n\n\n\n2012\n5.0\n\n\n2013\n9.0\n\n\n2014\n3.2\n\n\n2015\n3.8\n\n\n2016\n7.8\n\n\n2017\n6.7\n\n\n2018\n8.4\n\n\n2019\n8.4\n\n\n2020\n8.3\n\n\n2021\n8.3\n\n\n2022\n11.5\n\n\n\n\n  data2 |&gt; \n   mutate(words_in_name = str_count(name,\"\\\\b[^ ]+\\\\b\"),\n          num_commas = str_count(languages,\",\"),\n          num_languages = num_commas + 1,\n          publishedyear = as.numeric(publishedyear),\n          full_price = str_replace(full_price, \"NULL\", \"0\"),\n          full_price = as.numeric(full_price),\n          full_price = full_price/100) |&gt;\n   group_by(publishedyear) |&gt;\n   summarize(avg_words = mean(words_in_name)) |&gt;\n   filter(publishedyear &gt; 2006) |&gt;\n   kable(digits = 1) |&gt;\n   kable_styling(bootstrap_options = \"striped\", full_width = FALSE,\n                 position = \"float_right\")\n\nWarning: There were 3 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `num_commas = str_count(languages, \",\")`.\nCaused by warning in `stri_count_regex()`:\n! argument is not an atomic vector; coercing\nℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.\n\n\n\n\n\npublishedyear\navg_words\n\n\n\n\n2007\n3.8\n\n\n2008\n3.7\n\n\n2009\n3.7\n\n\n2010\n3.4\n\n\n2011\n3.2\n\n\n2012\n3.2\n\n\n2013\n3.2\n\n\n2014\n3.2\n\n\n2015\n3.0\n\n\n2016\n2.9\n\n\n2017\n2.9\n\n\n2018\n2.8\n\n\n2019\n2.8\n\n\n2020\n2.8\n\n\n2021\n2.8\n\n\n2022\n2.8\n\n\n\n\n\n\n#the json data waas very broken, not sure if these count as strings and regular expressions for my 3, but basically i'm just tricking r into turning into proper text\n descriptions &lt;- data2 |&gt; \n   mutate(names = str_extract(name, \".*\"),\n          descriptions = str_extract(description, \".*\")) |&gt;\n   select(descriptions, names, publishedyear)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `descriptions = str_extract(description, \".*\")`.\nCaused by warning in `stri_extract_first_regex()`:\n! argument is not an atomic vector; coercing\n\n tidy_descriptions &lt;- descriptions |&gt;\n  mutate(line = row_number()) |&gt;\n  unnest_tokens(word, descriptions, token = \"words\")\n   \nsmart_stopwords &lt;- get_stopwords(source = \"smart\")\n\nfont_stopwords &lt;- tibble(\n  word = c(\"br\",\"li\",\"strong\",\"ul\",\"quot\", \"game\"), \n  lexicon = \"font\")\n\n\n#top 20 words of all times\ntidy_descriptions |&gt;\n   filter(publishedyear != \"NULL\") |&gt;\n  anti_join(smart_stopwords) |&gt;\n  anti_join(font_stopwords) |&gt;\n  count(word, sort = TRUE) |&gt;\n  filter(word != \"NA\") |&gt;\n  slice_max(n, n = 20) |&gt;\n  ggplot(aes(fct_reorder(word, n), n)) +\n  geom_col() +\n  coord_flip()\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(word)`\n\n\n\n\n\n\n\n\n#top 10 word for each year (I tried so hard to order them for each year but it doesn't work )\ntidy_descriptions |&gt;\n   filter(publishedyear != \"NULL\",\n          publishedyear &gt; 2006) |&gt;\n  anti_join(smart_stopwords) |&gt;\n  anti_join(font_stopwords) |&gt;\n  group_by(publishedyear, word) |&gt;\n  summarize(n =n()) |&gt; \n  arrange(publishedyear, desc(n)) |&gt;\n  slice_max(n, n = 10) |&gt; \n  ggplot(aes(fct_reorder(word, n), n, fill = publishedyear)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ publishedyear, scales = \"free\")\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(word)`\n`summarise()` has grouped output by 'publishedyear'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\nwords &lt;- tidy_descriptions |&gt;\n  anti_join(stop_words) |&gt;\n  anti_join(font_stopwords) |&gt;\n  count(word) |&gt;\n  filter(word != \"NA\") |&gt;\n  arrange(desc(n))\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(word)`\n\n# word cloud of all years\nwordcloud(\n  words = words$word, \n  freq = words$n, \n  max.words = 200, \n  random.order = FALSE, \n  rot.per = 0.35,\n  scale = c(3.5, 0.25),\n  colors = brewer.pal(9, \"Dark2\"))\n\nWarning in brewer.pal(9, \"Dark2\"): n too large, allowed maximum for palette Dark2 is 8\nReturning the palette you asked for with that many colors"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aria’s Website!",
    "section": "",
    "text": "I really like R, and you should too!\nLearn more about me in “About Aria!”\nCheckout my mini projects from Data Science 2 in “Mini Projects” and my larger final projects in “Major Projects”\n\nI am also in St. Olaf Taiko! Come see us perform each spring and fall!"
  },
  {
    "objectID": "salmon_tummys.html",
    "href": "salmon_tummys.html",
    "title": "Diets of Juvenile Chinhook Salmon in PNW",
    "section": "",
    "text": "In Washington State, salmon are a big part of the water’s ecosystems and provide jobs and food to many people and animals. In elementary school most students spend time learning about the salmon cycle, even getting to raise their own fish. For this project we wanted to focus on something that was both interesting to us and held a bit of importance to our beloved home. Both of us were also intrigued by the prospect of this being someone’s job, going out and collecting this data. Over the course of this project we looked at the many different kinds of food that these juvenile chinhook salmon ate, how big they were,where these fish came from, and comparing the fish from hatcheries in various parts of the Pacific Northwest to the fish that were born in the wild. We examined the correlatory differences in the prey from both these sets of fish and the size of these fish.\n\n\n\nJuvenile Chinook salmon, Roger Tabor/USFWS, Public Domain, https://www.fws.gov/media/juvenile-chinook-salmon"
  },
  {
    "objectID": "salmon_tummys.html#weight-of-fish-and-prey-type",
    "href": "salmon_tummys.html#weight-of-fish-and-prey-type",
    "title": "Diets of Juvenile Chinhook Salmon in PNW",
    "section": "Weight of Fish and Prey Type",
    "text": "Weight of Fish and Prey Type\nIn this second graph we are exploring how the weight of a salmon correlates with the type of food it eats.\n\n\n\n\n\n\n\n\n\nThe fact that some of the heaviest fish ate plankton could be surprising, especially after cephalopods, a much more likely high median, is just above.\nThis graph is much like the previous one with all the same axes, though now the boxplots are split by fish that were raised in hatcheries vs. those in the wild.\n\n\n\n\n\n\n\n\n\nInterestingly, there is a strong correlation of wild fish being the heavier on average in all categories, and the hatchery raised median weights are much lighter, and nearly identical (~25 grams) across all categories, implying much more homogeneity in their group."
  },
  {
    "objectID": "salmon_tummys.html#length-of-fish-and-prey-type",
    "href": "salmon_tummys.html#length-of-fish-and-prey-type",
    "title": "Diets of Juvenile Chinhook Salmon in PNW",
    "section": "Length of Fish and Prey Type",
    "text": "Length of Fish and Prey Type\nFor our last size based graph we wanted to explore the length of the fish with how they correlate to the prey type eaten as compared to the weight graphs above. We also wanted to explore a graph with much more complication, and analyze it on many levels.\n\n\n\n\n\n\n\n\n\nThere’s a lot going on in this graph, but also some pretty interesting takeaways. The overall boxplots show that length has the same correlation with prey type as weight. The light gray beeswarm dots show that the hatchery fish once again are shorter than the whole group. Finally, the colored dots show the longest fish grouped by each hatchery."
  },
  {
    "objectID": "MiniMapProject.html",
    "href": "MiniMapProject.html",
    "title": "Mini Mapping Project",
    "section": "",
    "text": "Alt-text / Description\nThis is a chloropleth map of the United States of America, the X and Y axis are longitude and latitude, respectively. Alaska and Hawaii have been moved to the bottom left of the map for clarity of sight. Each state is filled with a color based on the percentage of change in federal land from 1990 to 2018. They range from -20% (a decrease in federal land) to 193% (a large increase in federal land). The biggest takeaway is that the majority of states have changed no more than 5% in the positive or negative direction. the pacific northwest seems to have gained land slightly, while the southwest states have all lost land, with Arizona having decreased the most in the country. The eastern side of the US, has largely made gains from 1% up to 70% in Maine.\nStatic Plot\n\n\n\n\n\n\n\n\n\nInteractive Plot\n\n\n\n\n\n\n\n\nDescription\nA major insight from this map is that many regions of states choose birds within the same family, implying that bird likely lives within that area. this is especially true of the Icteridae (Meadow larks) in the great plains, Mimidae (mocking birds) in the south east, and Cardinalidae (Cardinals) in the central eastern region. All 3 of these birds have high concentrations in these areas, so their states seem to have chosen a local bird that they feel represents them.\nStatic Plot\n\n\n\n\n\n\n\n\n\nInteractive Plot"
  }
]